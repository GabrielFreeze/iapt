{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2963b359",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2290e2e",
   "metadata": {},
   "source": [
    "Preprocessing is about transforming the data of the corpus into a usable format. Currently, the korpus data is stored in multiple text file in different folders (Academic, Culture, ...). These text files will be joined and transformed into a .csv file. This is because .csv files can easily be loaded into a Pandas DataFrame which is optimised for data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98645cd1",
   "metadata": {},
   "source": [
    "The original korpus is over 6GB in size. Most of this data comes from the Parliament and Law Subjects. In order to reduce complexity, a subset of the data in these subjects will be considered. Some of the data left out will be used as a test corpus. \n",
    "\n",
    "The test corpus only features 5 sentences taken from the Parliament section. This is because higher sentences produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbb0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56b112a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder = os.path.join(os.getcwd(),'..','data','korpus')\n",
    "subjects = ['Academic','Culture','European','Law','News',\n",
    "            'Opinion','Parliament','Religion','Sport','Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9504e1",
   "metadata": {},
   "source": [
    "## Removing XML tags from <i>korpus</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32caafa9",
   "metadata": {},
   "source": [
    "The text files contain different XML tags that indicate the start of sentences, paragraphs etc. These XML tags create complexity within the dataset. We only really need the start and end of sentences, since they will be further utilised in Text Generation.\n",
    "\n",
    "The following regex is applied to every text file, which removes all XML tags except <s>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8c9f2",
   "metadata": {},
   "source": [
    "All tags except <b>\\<s\\></b> will be removed from the korpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727c7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'<(?![/]?s).*?>')\n",
    "def clean(text):\n",
    "    return regex.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab5052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic.csv\n",
      "Academic1.txt\n",
      "Academic2.txt\n",
      "Culture.csv\n",
      "Culture1.txt\n",
      "Culture2.txt\n",
      "European.csv\n",
      "European.txt\n",
      "Law.csv\n",
      "Law1.txt\n",
      "News.csv\n",
      "News1.txt\n",
      "News2.txt\n",
      "News3.txt\n",
      "News4.txt\n",
      "News5.txt\n",
      "News6.txt\n",
      "News8.txt\n",
      "Opinion.csv\n",
      "Opinion1.txt\n",
      "Opinion2.txt\n",
      "Parliament.csv\n",
      "Parliament2.txt\n",
      "Religion.csv\n",
      "Religion1.txt\n",
      "Religion2.txt\n",
      "Sport.csv\n",
      "Sport1.txt\n",
      "Sport2.txt\n",
      "Test.csv\n",
      "Test.txt\n"
     ]
    }
   ],
   "source": [
    "for sbj in subjects:\n",
    "    path = os.path.join(folder,sbj)\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        filepath = os.path.join(path,filename)\n",
    "\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            print(filename)\n",
    "            text = f.read()\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(clean(text))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146385c",
   "metadata": {},
   "source": [
    "## Turning text file into dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8641b7",
   "metadata": {},
   "source": [
    "Next we will transform every text file pertaining to a subject into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e26bed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic.csv\n",
      "Academic1.txt\n",
      "Academic2.txt\n",
      "Culture.csv\n",
      "Culture1.txt\n",
      "Culture2.txt\n",
      "European.csv\n",
      "European.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-232c2e73ecd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                 \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;31m#Start and End Token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sbj in subjects:\n",
    "    data = []\n",
    "    path = os.path.join(folder,sbj)\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        filepath = os.path.join(path,filename)\n",
    "        \n",
    "        with open(filepath,'r', encoding='utf-8') as f:\n",
    "            print(filename)\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            for l in lines:\n",
    "                d = l.split('\\t')\n",
    "                \n",
    "                #Start and End Token\n",
    "                if len(d) == 1:\n",
    "                    if re.search(r'<s id=\"[0-9]*\">', d[0]): data.append(['<s>','START',None,None])\n",
    "                    elif re.search(r'</s>', d[0]):          data.append(['</s>','END',None,None])\n",
    "                \n",
    "                elif len(d) > 1:\n",
    "                    d[-1] = d[-1][:-1]\n",
    "                    data.append(d)\n",
    "                    \n",
    "    df = pd.DataFrame(data, columns=['Word','POS','Lemma','Root'])\n",
    "    df.to_csv(os.path.join(path,sbj+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ad781",
   "metadata": {},
   "source": [
    "## Joining individual csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207e0a5",
   "metadata": {},
   "source": [
    "Now, the individual csv files will be concatanated together. The result is a massive spreadsheet which encodes all the corpus's information. This is done so we can easily load the entire korpus into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df8658",
   "metadata": {},
   "source": [
    "We will create 2 versions of korpus.csv. One will simply be all the csv files together while the other will read an equal number of bytes from each file. This is so the frequency counts won't be biased based on the subject of the texts.\n",
    "\n",
    "Note that we do not include the text file in the training korpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['Academic','Culture','European','Law','News',\n",
    "            'Opinion','Parliament','Religion','Sport',]\n",
    "\n",
    "with open(os.path.join(folder,'korpus.csv'), 'w', encoding='utf-8') as f1:\n",
    "    for sbj in subjects:\n",
    "        with open(os.path.join(folder,sbj,sbj+'.csv'), 'r', encoding='utf-8') as f2:\n",
    "            f1.write('\\n')\n",
    "            f1.write(f2.read())\n",
    "            print(f'{sbj} Finished')\n",
    "\n",
    "print('All Finished\\n')\n",
    "\n",
    "\n",
    "min_size = min([os.path.getsize(os.path.join(folder,sbj,sbj+'.csv')) for sbj in subjects])\n",
    "\n",
    "#Only read min_length lines from each csv file. \n",
    "\n",
    "with open(os.path.join(folder,'norm_korpus.csv'), 'w', encoding='utf-8') as f1:\n",
    "    for sbj in subjects:\n",
    "        with open(os.path.join(folder,sbj,sbj+'.csv'), 'r', encoding='utf-8') as f2:    \n",
    "            f1.write('\\n')\n",
    "            f1.write(f2.read(min_size))\n",
    "            print(f'{sbj} Finished')\n",
    "\n",
    "print('All Finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c734120",
   "metadata": {},
   "source": [
    "We can confirm that the csv files have been joined by summing the size of the individual files and comparing it to the aggregate one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7a035",
   "metadata": {},
   "source": [
    "## Exploring Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb382ae4",
   "metadata": {},
   "source": [
    "Now that the corpus is in an acccessible format, we can load it into a Pandas DataFrame. \n",
    "We will not use all the information in the Corpus. We only really need access to the Word column, so POS Lemma and Root can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8ad084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>START</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L-</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>għan</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prinċipali</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ta'</td>\n",
       "      <td>GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Conectando</td>\n",
       "      <td>NOUN-PROP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mundos</td>\n",
       "      <td>NOUN-PROP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(</td>\n",
       "      <td>X-PUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Malta</td>\n",
       "      <td>NOUN-PROP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>)</td>\n",
       "      <td>X-PUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word        POS\n",
       "0         <s>      START\n",
       "1          L-        DEF\n",
       "2        għan       NOUN\n",
       "3  prinċipali        ADJ\n",
       "4         ta'        GEN\n",
       "5  Conectando  NOUN-PROP\n",
       "6      Mundos  NOUN-PROP\n",
       "7           (      X-PUN\n",
       "8       Malta  NOUN-PROP\n",
       "9           )      X-PUN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_all = pd.read_csv(os.path.join(folder,'korpus.csv'),\n",
    "                 usecols=[\"Word\",\"POS\"],\n",
    "                 dtype={\"Word\": \"U\",\"POS\": \"S\"},\n",
    "                 nrows=50_000_000)\n",
    "\n",
    "df_norm = pd.read_csv(os.path.join(folder,'norm_korpus.csv'),\n",
    "                      usecols=[\"Word\",\"POS\"],\n",
    "                      dtype={\"Word\": \"U\",\"POS\": \"S\"})\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(folder,'Test','Test.csv'),\n",
    "                      usecols=[\"Word\",\"POS\"],\n",
    "                      dtype={\"Word\": \"U\",\"POS\": \"S\"})\n",
    "\n",
    "\n",
    "df = [df_all,df_norm,df_test]\n",
    "\n",
    "\n",
    "\n",
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62ff14",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b053cf",
   "metadata": {},
   "source": [
    "The corpus is not an accurate representation of the Maltese Language. The POS tags indicate the context of words within the sentence. We will clean the corpus by omitting certain types of words.\n",
    "\n",
    "The Maltese Tagset can be found here: https://mlrs.research.um.edu.mt/resources/malti03/tagset30.html\n",
    "\n",
    "Giberrish words need to be removed as they do not make part of coherent speech.\n",
    "Interjection words break the flow of sentences and hence does not encode.\n",
    "Punctuation is not necessary to encode valid sentences. Note: The end of a sentence is represented by </s/>\n",
    "\n",
    "English words were deliberately left in since they can be incorporated in a maltese sentence and removing them would break the sentence's flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e312aca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['START', 'DEF', 'NOUN', 'ADJ', 'GEN', 'NOUN-PROP', 'X-PUN',\n",
       "       'X-ABV', 'PREP', 'CONJ-CORD', 'PART-PASS', 'PREP-DEF', 'PRON-PERS',\n",
       "       'COMP', 'VERB', 'END', 'LIL-DEF', 'CONJ-SUB', 'KIEN', 'GEN-PRON',\n",
       "       'ADV', 'VERB-PSEU', 'NEG', 'GEN-DEF', 'QUAN', 'PRON-DEM', 'X-DIG',\n",
       "       'PRON-INT', 'FOC', 'PREP-PRON', 'NUM-WHD', 'LIL', 'NUM-CRD',\n",
       "       'X-ENG', 'X-FOR', 'PROG', 'INT', 'X-BOR', 'PRON-PERS-NEG',\n",
       "       'LIL-PRON', 'PRON-INDEF', 'PRON-DEM-DEF', 'NUM-ORD', 'HEMM',\n",
       "       'PRON-REF', 'PART-ACT', 'FUT', 'NUM-FRC', 'PRON-REC', 'POS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['POS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6165d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df[i] = df[i].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dba331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Maltese Tagset: https://mlrs.research.um.edu.mt/resources/malti03/tagset30.html\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df[i] = df[i].drop(df[i][df[i]['POS']=='X-PUN'].index) #Punctuation\n",
    "    df[i] = df[i].drop(df[i][df[i]['POS']=='X-BOR'].index) #Gibberish\n",
    "    df[i] = df[i].drop(df[i][df[i]['POS']=='INT'].index)   #Interjections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3984b7",
   "metadata": {},
   "source": [
    "### Removing semi-colons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0aebd",
   "metadata": {},
   "source": [
    "Semi colons will be used as delimters in ngrams later on. Having semicolons at the beginning or end of a word disrupts the delimiting process. Although individual semicolons should have been removed in the previous stage, not all semicolons were tagged properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6904fc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  <s>\n",
       "1                   L-\n",
       "2                 għan\n",
       "3           prinċipali\n",
       "4                  ta'\n",
       "               ...    \n",
       "49999994         Bajda\n",
       "49999996          </s>\n",
       "49999997           <s>\n",
       "49999998            Dr\n",
       "49999999        Zammit\n",
       "Name: Word, Length: 44232071, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c61c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df[i]['Word'] = df[i]['Word'].apply(lambda s: ''.join(str(s).split(';')), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d1dfc",
   "metadata": {},
   "source": [
    "Finally we will remove all NA values. Other special cases should also be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff413c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df[i] = df[i].dropna(subset=['Word'])\n",
    "    df[i] = df[i].drop(df[i][df[i][\"Word\"]=='\"'].index)\n",
    "    df[i] = df[i].drop(df[i][df[i][\"Word\"]=='&lt'].index)\n",
    "    df[i] = df[i].drop(df[i][df[i][\"Word\"]=='&gt'].index)\n",
    "    df[i] = df[i].drop(df[i][df[i][\"Word\"]=='&amp'].index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5df67",
   "metadata": {},
   "source": [
    "We save the dataframes into a csv file so that they can be loaded back into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7884382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all  = df[0]\n",
    "df_norm = df[1]\n",
    "df_test = df[2]\n",
    "\n",
    "df_all.to_csv(os.path.join(folder,'korpus_clean.csv'), index=False)\n",
    "df_norm.to_csv(os.path.join(folder,'norm_korpus_clean.csv'), index=False)\n",
    "df_test.to_csv(os.path.join(folder,'Test','Test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628b596",
   "metadata": {},
   "source": [
    "## Frequency Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f7da5",
   "metadata": {},
   "source": [
    "We will also create frequency counts for all unique words in both versions of the korpus. This will be used later when generating the ngram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7c2c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(folder, \"korpus_clean.csv\"),\n",
    "                 usecols=[\"Word\",\"POS\"],\n",
    "                 dtype={\"Word\": \"U\",\"POS\": \"S\"})\n",
    "\n",
    "df_norm = pd.read_csv(os.path.join(folder, \"norm_korpus_clean.csv\"),\n",
    "                 usecols=[\"Word\",\"POS\"],\n",
    "                 dtype={\"Word\": \"U\",\"POS\": \"S\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4aa3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>START</td>\n",
       "      <td>1791836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>END</td>\n",
       "      <td>1791835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l-</td>\n",
       "      <td>DEF</td>\n",
       "      <td>1584004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>li</td>\n",
       "      <td>COMP</td>\n",
       "      <td>1391328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ta'</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1379784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491718</th>\n",
       "      <td>banama</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491719</th>\n",
       "      <td>banalment</td>\n",
       "      <td>ADV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491720</th>\n",
       "      <td>banalizzat</td>\n",
       "      <td>PART-PASS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491721</th>\n",
       "      <td>banakrella</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491722</th>\n",
       "      <td>diz</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491723 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word        POS  Frequency\n",
       "0              <s>      START    1791836\n",
       "1             </s>        END    1791835\n",
       "2               l-        DEF    1584004\n",
       "3               li       COMP    1391328\n",
       "4              ta'        GEN    1379784\n",
       "...            ...        ...        ...\n",
       "491718      banama       NOUN          1\n",
       "491719   banalment        ADV          1\n",
       "491720  banalizzat  PART-PASS          1\n",
       "491721  banakrella       NOUN          1\n",
       "491722         diz        ADJ          1\n",
       "\n",
       "[491723 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all unique words\n",
    "df_frequency = df.value_counts().to_frame()[0].reset_index()\n",
    "df_frequency.columns = ['Word','POS','Frequency']\n",
    "\n",
    "\n",
    "df_normal_frequency = df_norm.value_counts().to_frame()[0].reset_index()\n",
    "df_normal_frequency.columns = ['Word','POS','Frequency']\n",
    "\n",
    "df_frequency.to_csv(os.path.join(folder,'korpus_frequency.csv'), index=False)\n",
    "df_normal_frequency.to_csv(os.path.join(folder,'norm_korpus_frequency.csv'), index=False)\n",
    "\n",
    "df_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780a50e",
   "metadata": {},
   "source": [
    "We can validate that this is working as the top frequent words are words which maltese native speakers expect to be at the top, words such as <i>ta'</i> and <i>l-</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35649762",
   "metadata": {},
   "source": [
    "#### Question 1. How large is the corpus?\n",
    "The normalised corpus only contains about 125k occurences of words. For the sake of brevity, only 30 million occurences of words were considered from the non-normalised korpus when cleaning and generating the frequency counts. This was because loading the whole data and performing operations on it would take a long time and be computationally expensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ari2203-venv",
   "language": "python",
   "name": "ari2203-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
